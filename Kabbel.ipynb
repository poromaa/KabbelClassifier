{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Käbbel importerare\n",
    "\n",
    "Denna funktion läser allt käbbel från regeringens hemsida och sparar ner skiten i en fil: kabbel.txt\n",
    "- datasize kan sättas till hur många sidor som ska hämtas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Läs in fil och skapa ordvektor\n",
    "\n",
    "Denna läser in orden från ovan skapade fil och bygger en data av den som vi använder som träningsdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import urllib.request, json, re\n",
    "import pprint as pp\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "import random\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_web_data(datasize=3):\n",
    "\n",
    "    kabbel = {}\n",
    "    talare = \"\"\n",
    "    #\"http://data.riksdagen.se/anforandelista/?rm=&anftyp=Nej&d=&ts=&parti=s&iid=0218878014918&sz=\"+str(datasize)+\"&utformat=json\"\n",
    "    #http://data.riksdagen.se/anforandelista/?rm=&anftyp=&d=&ts=&parti=&iid=&sz=10&utformat=xml\n",
    "    data = []\n",
    "    with urllib.request.urlopen(\"http://data.riksdagen.se/anforandelista/?rm=&anftyp=&d=&ts=&parti=&iid=&sz=\"+str(datasize)+\"&utformat=json\") as url:\n",
    "        rawdata = json.loads(url.read().decode())\n",
    "        anflist = rawdata[\"anforandelista\"]['anforande']\n",
    "        for anf in anflist:\n",
    "            with urllib.request.urlopen(anf['anforande_url_xml'] + \"/json\") as url:\n",
    "                anforande = json.loads(url.read().decode())\n",
    "\n",
    "                kbl = re.sub(' +',' ',re.sub(\"(<p>|</p>|Herr talman!|\\(Applåder\\))\",\" \",(anforande[\"anforande\"][\"anforandetext\"]))) \n",
    "               # meningar = re.findall('(?:\\d[,.]|[^,.])*(?:[,.]|$)', kbl) #kbl.split(r\"\\.|,\");                \n",
    "                meningar = re.findall('(?:\\d[,.\\?!-]|[^,.\\?!-])*(?:[,.\\?!-]|$)', kbl) #kbl.split(r\"\\.|,\");\n",
    "\n",
    "\n",
    "                for mening in meningar:\n",
    "                    if mening.strip()!=\"\":                \n",
    "                        data_point = {}\n",
    "                        data_point['talare'] = anforande[\"anforande\"]['talare']\n",
    "                        data_point['parti'] = anforande[\"anforande\"]['parti']\n",
    "                        data_point['kabbel'] = mening.strip().lower()\n",
    "                        data.append(data_point)\n",
    "\n",
    "    with open('kabbel_new.txt','w',encoding='utf-8') as f:\n",
    "        f.write(json.dumps(data, sort_keys=True, indent=4))\n",
    "    return data\n",
    "\n",
    "\n",
    "#data_ex = import_web_data(datasize=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def file_to_json(filename):\n",
    "    with open(filename,encoding=\"utf-8\") as f:\n",
    "        data = json.loads(f.read())\n",
    "        return data\n",
    "\n",
    "def kabbel_to_vec(jsondata):\n",
    "    speech = []\n",
    "    classes = []\n",
    "    person = []\n",
    "    words = []\n",
    "    for sample in jsondata:\n",
    "        cleaned = re.sub('[\\.|,|?|!|\\-|\"\\'|\\(|\\)|;|:]', ' ', sample[\"kabbel\"])\n",
    "        words += cleaned.lower().split()\n",
    "        speech.append(cleaned)\n",
    "        classes.append(sample[\"parti\"])\n",
    "        person.append(sample[\"talare\"])\n",
    "\n",
    "    #print(speech)\n",
    "    #data = tf.compat.as_str(speech).split()\n",
    "    return speech, classes, person\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Läs in olika delar och tilldela X och Y i vår modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jsondata = file_to_json(\"kabbel_new.txt\")\n",
    "print(\"Exempel på ett sample ur datan: \\n\"+json.dumps(jsondata[5:9], indent=4))\n",
    "\n",
    "\n",
    "speech, classes, person = kabbel_to_vec(jsondata)\n",
    "\n",
    "print(\"Classes: {0}... \\n\\nSpeech:{1}...\".format(classes[5:9], speech[5:9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Försök till lemmatisering genom egen metod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexOf(text, elem):\n",
    "        try:\n",
    "            thing_index = text.index(elem)\n",
    "            return thing_index\n",
    "        except ValueError:\n",
    "            return -1\n",
    "\n",
    "def lemmatize(textList):        \n",
    "        \n",
    "    all_words = \"\".join(textList).split(' ');\n",
    "    myset = set(all_words)\n",
    "    all_words = list(myset)\n",
    "\n",
    "    all_words.sort( reverse=False)\n",
    "    #all_words.sort(key=len, reverse=False)\n",
    "    tmp = 10\n",
    "    start = 300\n",
    "    #print(all_words[start:start+tmp])\n",
    "    orig = all_words #[start:start+tmp]\n",
    "    test = all_words #[start:start+tmp]\n",
    "\n",
    "    #res = [a for (a,i) in test]\n",
    "    tid =  enumerate(test);\n",
    "    #list2 = [test[idx] for idx, w in tid if test[idx+1] ==w]\n",
    "    newlist = []\n",
    "    for i in range(len(test)):\n",
    "        fidx = 0\n",
    "        if(len(newlist)>0):\n",
    "            fidx = (indexOf(test[i],newlist[i-1])+1)\n",
    "        if(fidx != 0 and (1.0*len(newlist[i-1])/len(test[i]) >=.5)):\n",
    "            newlist.append(newlist[i-1])\n",
    "        else:\n",
    "            newlist.append(test[i])\n",
    "\n",
    "    print(all_words[start:start+tmp])\n",
    "    print(newlist[start:start+tmp])\n",
    "    return dict(list(zip(orig,newlist)))\n",
    "lem_dict = lemmatize(speech)\n",
    "\n",
    "teint = 325\n",
    "print(speech[teint])\n",
    "\n",
    "def replace_words(speechlist, words):\n",
    "    out_speechlist = []\n",
    "    for sentence in speechlist:\n",
    "        new_sentence = ''\n",
    "        for word in sentence.split(' '):\n",
    "            if word != ' ':\n",
    "                new_sentence+= words[word] + ' '\n",
    "        out_speechlist.append(new_sentence)\n",
    "    return out_speechlist    \n",
    "\n",
    "sentence = [speech[teint]]\n",
    "regering = {'regeltolkningar': 'regeltolkningar', 'regelverk': 'regelverk', 'regelverken': 'regelverk', 'regelverket': 'regelverk', 'regeländringarna': 'regeländringarna', 'regera': 'regera', 'regerade': 'regera', 'regerar': 'regera', 'regering': 'regering', 'regeringar': 'regering', 'regeringarna': 'regering', 'regeringen': 'regering', 'regeringens': 'regering', 'regerings': 'regering', 'regeringsbeslut': 'regering', 'regeringsbildare': 'regering', 'regeringsformen': 'regering', 'regeringsföreträdare': 'regering', 'regeringsföreträdarna': 'regering', 'regeringsförklaring': 'regering', 'regeringsförklaringen': 'regering', 'regeringskamrater': 'regering', 'regeringskansli': 'regering', 'regeringskansliet': 'regering', 'regeringskansliets': 'regering', 'regeringskanslipersonal': 'regering', 'regeringskretsen': 'regering', 'regeringskris': 'regering', 'regeringsmakten': 'regering', 'regeringsnivå': 'regering', 'regeringsombildning': 'regering', 'regeringsparti': 'regering', 'regeringspartierna': 'regering', 'regeringspartiet': 'regering', 'regeringspartner': 'regering', 'regeringsperiod': 'regering', 'regeringsperioder': 'regering', 'regeringssammanträde': 'regering', 'regeringssammanträdet': 'regering', 'regeringssidan': 'regering', 'regeringsskiftet': 'regering', 'regeringsställning': 'regering', 'regeringstid': 'regering', 'regeringsunderlaget': 'regering', 'regeringsuppdrag': 'regering', 'regeringsåren': 'regering'}\n",
    "\n",
    "\n",
    "print(replace_words(speech, lem_dict))\n",
    "\n",
    "\n",
    "\n",
    "#pattern = '|'.join(sorted(re.escape(k) for k in lem_dict))\n",
    "#result =  re.sub(pattern, lambda m: lem_dict.get(m.group(0).upper()), speech[teint])\n",
    "#pattern = re.compile(r'\\b(' + '|'.join(lem_dict.keys()) + ')\\b')\n",
    "#result = pattern.sub(lambda x: lem_dict[x.group()], speech[teint])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anpassning av träningsdata (sample=X) och klassificering (Y)\n",
    "\n",
    "Vi konverterar varje sample till en träningsvektor av längden `len(unika antal ord)` och värde `n` för antal förekomst per ord\n",
    "\n",
    "Vi konverterar varje klasificering till en vektor som beskriver klasserna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def samples_to_vocab(samples):\n",
    "    all_unique_words = list(set(' '.join(samples).split(' '))) \n",
    "    word_to_idx = {i:w for w,i in enumerate(all_unique_words)}\n",
    "    idx_to_word = {i:w for i,w in enumerate(all_unique_words)}\n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "def sample_to_vec(sample, wi):\n",
    "    vocabular = wi\n",
    "    sample_words = np.array(sample.split(\" \"))\n",
    "    vec = np.array([wi[x] for x in sample_words])\n",
    "    one_hot = np.zeros([len(vec), len(vocabular)])\n",
    "    one_hot[np.arange(len(vec)),vec] = 1\n",
    "    one_hot = np.sum(one_hot,axis=0)\n",
    "    return vec,one_hot\n",
    "\n",
    "def samples_to_train(samples):\n",
    "    vocabulary, iw = samples_to_vocab(samples)\n",
    "    sample_vector = []\n",
    "    for sample in samples:\n",
    "        _, x = sample_to_vec(sample, vocabulary)\n",
    "        sample_vector.append(x)\n",
    "    return np.matrix(sample_vector).reshape((len(samples),len(vocabulary))).T, vocabulary, iw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data =[\n",
    "    {\n",
    "        \"talare\": \"Joakim\",\n",
    "        \"parti\": \"S\",\n",
    "        \"kabbel\": \"jag ska, fatta mig kort.\"\n",
    "    },\n",
    "    {\n",
    "        \"talare\": \"Björn\",\n",
    "        \"parti\": \"SD\",\n",
    "        \"kabbel\": \"jag har redan redogjort f\\u00f6r huvudargumentationen bakom detta,\"\n",
    "    }\n",
    "]\n",
    "\n",
    "test_samples,test_parti, test_classes = kabbel_to_vec(test_data)\n",
    "\n",
    "#test_samples = [\"Apan är Bäst\", \"Joakim är Bäst i klassen i alla fall\",\"Joakim\"]\n",
    "#test_classes = [\"Apan\", \"Joakim\",\"Joadkim\"]\n",
    "\n",
    "print(test_samples)\n",
    "a, b, c = samples_to_train(test_samples)\n",
    "print(a)\n",
    "samples_to_train(test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_data_info(speech,classes,test_index=0):\n",
    "    X_train,vocabulary_x, _ = samples_to_train(speech)\n",
    "    Y_train,vocabulary_y, _ = samples_to_train(classes)\n",
    "    print(\"Antal unika ord (features X):\\t\",X_train.shape[0])\n",
    "    print(\"training samples m:\\t\\t\",Y_train.shape[1])\n",
    "    print(\"unika klassificeringar (Y):\\t\",Y_train.shape[0])\n",
    "    vec, one = sample_to_vec(speech[test_index],vocabulary_x)\n",
    "    print(\"X:\")\n",
    "    print(\"Original: {0}{1}\".format(speech[test_index][:70],\"...\" if len(speech[test_index])>70 else \"\"))\n",
    "    print(\"Vokabulär: {0}\".format([x for x in vocabulary_x][:10]))\n",
    "    print(\"x0 :\\n{0}\".format(vec))\n",
    "    print(\"x0 compressed:\\n{0}\\n\".format(X_train)) \n",
    "    print(\"Y:\")\n",
    "    print([x for x in vocabulary_y])\n",
    "    print(\"y0 compressed:\\n{0}\".format(Y_train)) \n",
    "    assert X_train.shape[1] == Y_train.shape[1] , 'Det måste finnas lika många X som Y:n'\n",
    "\n",
    "def print_shape_info(X_train,Y_train):\n",
    "    print(\"number of training examples: {0}\".format(X_train.shape[1]))\n",
    "    print(\"X_train shape: {0}\".format(str(X_train.shape)))\n",
    "    print(\"Y_train shape: {0}\".format(str(Y_train.shape)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = [\"Apan är Bäst Bäst Bäst\", \"Joakim är Bäst i klassen i alla fall\",\"Joakim\",\"Hejhej\",\"Joakim\"]\n",
    "test_classes = [\"Apan\", \"Joakim\",\"Joadkim\",\"Annan\",\"Joakim\"]\n",
    "\n",
    "\n",
    "\n",
    "X_samp,vocabulary_x, iw = samples_to_train(test_samples)\n",
    "Y_samp,vocabulary_y ,iw= samples_to_train(test_classes)\n",
    "\n",
    "print_shape_info(X_samp, Y_samp)\n",
    "print_data_info(test_samples,test_classes,test_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stop_words(X, iw, top=10):\n",
    "\n",
    "    X_samp_sum = np.sum(X,axis=1)\n",
    "    \n",
    "    idx = np.argsort(-X_samp_sum, axis=0)\n",
    "    iw_ar = [w for (i,w) in iw.items()]\n",
    "    count = X_samp_sum[idx[:top]].flatten()\n",
    "    labels = np.asarray(iw_ar)[idx[:top]]\n",
    "    a = count.tolist()[0]\n",
    "    b = labels.flatten().tolist()\n",
    "    z = dict(zip(b,a))\n",
    "    res = []\n",
    "    for k,v in z.items():\n",
    "        res.append(\"{0}\\t{1:20}\\t[{2}]\".format(len(res)+1,k,v).expandtabs(2))\n",
    "    print(\"\\n\".join(res))\n",
    "    return idx\n",
    "\n",
    "def remove_features(X_samp, del_idx, iw, remove):\n",
    "    #del_idx = get_stop_words(X_samp,iw,remove)\n",
    "    iwr = [w for (i,w) in iw.items()]\n",
    "    iwr = np.delete(iwr, del_idx[:remove], axis=0)\n",
    "    X_red = np.delete(X_samp, del_idx[:remove], axis=0)\n",
    "    \n",
    "    assert X_samp.shape[0] == X_red.shape[0]+remove\n",
    "    return X_red, iwr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = 2\n",
    "\n",
    "X_samp,_, iw = samples_to_train(test_samples)\n",
    "del_idx = get_stop_words(X_samp,iw,remove)\n",
    "X_red, iwr = remove_features(X_samp,del_idx, iw, remove)\n",
    "\n",
    "print(iwr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(X,Y, seed=1):\n",
    "    np.random.seed(1)\n",
    "    randomize = np.arange(X.shape[1])\n",
    "    np.random.shuffle(randomize)\n",
    "    #blanda kolumnerna\n",
    "    X = X[:,randomize]\n",
    "    Y = Y[:,randomize]\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "a = np.random.rand(2,5)\n",
    "b,c  = shuffle(a,a,1)\n",
    "pp.pprint(a)\n",
    "pp.pprint(b)\n",
    "pp.pprint(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_dev_test(X,Y,ratio=.5):\n",
    "    np.random.seed(2)\n",
    "    m = X.shape[1]\n",
    "    m_train = int(np.floor((X.shape[1]*ratio)))\n",
    "    m_bi = int((m - m_train)/2)\n",
    "    \n",
    "    X_orig, Y_orig = shuffle(X,Y)\n",
    "    \n",
    "    X_train, X_dev, X_test = X_orig[:,:m_train], X_orig[:,m_train:m_train+m_bi], X_orig[:,m_train+m_bi:]\n",
    "    Y_train, Y_dev, Y_test = Y_orig[:,:m_train], Y_orig[:,m_train:m_train+m_bi], Y_orig[:,m_train+m_bi:]\n",
    "    print(m_train,m_bi+m_train)\n",
    "    return X_train, X_dev, X_test, Y_train, Y_dev, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print_data_info(speech,classes)\n",
    "x_train,vocabulary_x,_ = samples_to_train(test_samples)\n",
    "y_train,vocabulary_y,_ = samples_to_train(test_classes)\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = split_train_dev_test(x_train,y_train,ratio=.9)\n",
    "m = x_train.shape[1]\n",
    "print('m:{0}'.format(m))\n",
    "print('m_train:{0}'.format(X_train.shape[1]))\n",
    "print('m_dev:{0}'.format(X_dev.shape[1]))\n",
    "print('m_test:{0}'.format(X_test.shape[1]))\n",
    "\n",
    "print(x_train)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "assert m == Y_train.shape[1]+Y_dev.shape[1] + Y_test.shape[1]\n",
    "assert m == X_train.shape[1]+X_dev.shape[1] +X_test.shape[1] #\"matrisen harinte samma features\"\n",
    "assert X_train.shape[0] == X_dev.shape[0] #matrisen behåller formen features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_x,n_y):\n",
    "    X = tf.placeholder(shape=[n_x,None],dtype=tf.float32,name=\"X\")\n",
    "    Y = tf.placeholder(shape=[n_y,None],dtype=tf.float32,name=\"Y\")\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiera parametrar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params(hyperparameters):\n",
    "    \"\"\"Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [25, 12288]\n",
    "                        b1 : [25, 1]\n",
    "                        W2 : [12, 25]\n",
    "                        b2 : [12, 1]\n",
    "                        W3 : [6, 12]\n",
    "                        b3 : [6, 1]\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\"\"\"\n",
    "    \n",
    "    layer_dims = hyperparameters['layer_dims']\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = tf.get_variable(name=\"W\"+str(l),shape=[layer_dims[l],layer_dims[l-1]],initializer=tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "        parameters['b' + str(l)] = tf.get_variable(name=\"b\"+str(l),shape=[layer_dims[l],1],initializer=tf.zeros_initializer())\n",
    "\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_hyperparams(layer_dims):\n",
    "    hyperparameters = {}\n",
    "    hyperparameters['layer_dims'] = layer_dims\n",
    "    \n",
    "    return hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph() #nollställ modell\n",
    "hyperparameters = init_hyperparams(layer_dims=[X_train.shape[0],25,12,Y_train.shape[0]])\n",
    "paramteters = initialize_params(hyperparameters)\n",
    "\n",
    "pp.pprint(paramteters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, hyperparameters, parameters):\n",
    "    layer_dims = hyperparameters['layer_dims']\n",
    "    L = len(layer_dims)-1\n",
    "  \n",
    "    Zi = {}\n",
    "    Ai = {}\n",
    "    Ai[0] = X\n",
    "    for l in range(1,L+1):\n",
    "        Zi[l] = tf.matmul(parameters['W'+str(l)],Ai[l-1])+parameters['b'+str(l)]\n",
    "        Ai[l] = tf.nn.relu(Zi[l])\n",
    "        #print(\"ff\"+str(l)+\" \"+'W'+str(l)+\"x\"+\"Ai[\"+str(l-1)+\"] + \"+'b'+str(l))\n",
    "\n",
    "    return Zi[L]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(X_train.shape[0], Y_train.shape[0])\n",
    "    parameters = initialize_params(hyperparameters)\n",
    "    Z3 = forward_propagation(X, hyperparameters, parameters)\n",
    "    print(\"Z3 = \" + str(Z3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(X_train.shape[0], Y_train.shape[0])\n",
    "    parameters = initialize_params(hyperparameters)\n",
    "    Z3 = forward_propagation(X, hyperparameters, parameters)\n",
    "    print(\"Z3 = \" + str(Z3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kostnadsfunktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    \n",
    "    #    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    #biases = tf.Variable(tf.zeros([num_labels]))\n",
    "   # weights = tf.Variable(tf.truncated_normal([Z3.shape[0], Z3.shape[1]]))\n",
    "   # biases = tf.Variable(tf.zeros([Z3.shape[1]]))\n",
    "  \n",
    "    # Training computation.\n",
    "   # logits = tf.transpose(tf.matmul(Z3, weights) + biases )\n",
    "    logits = tf.transpose(Z3 + Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    # L2_regularization_cost = lambd/(2*m)*(np.sum(np.square(W1))+np.sum(np.square(W2))+np.sum(np.square(W3)))\n",
    "    #kostnadsfunktion ej regulariserad.\n",
    "    #logits = tf.matmul(tf_train_dataset, weights) + biases \n",
    "    # Original loss function\n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) )\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels))\n",
    "    \n",
    "    return cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(Z3, Y, parameters, lambd):\n",
    "\n",
    "    layer_dims = hyperparameters['layer_dims']\n",
    "    L = len(layer_dims)-1\n",
    "    L2_regularization_cost = tf.Variable(0,dtype=tf.float32)\n",
    "    for l in range(1,L):\n",
    "        L2_regularization_cost +=  tf.nn.l2_loss(parameters['W'+str(l)]) \n",
    "\n",
    "\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels)\n",
    "    \n",
    "    cost = tf.reduce_mean(loss + lambd*L2_regularization_cost)\n",
    "\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = hyperparameters['layer_dims']\n",
    "L = len(layer_dims)-1\n",
    "for l in range(1,L):\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X_train,vocabulary_x, iw = samples_to_train(test_samples)\n",
    "Y_train,vocabulary_y ,iw= samples_to_train(test_classes)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(X_train.shape[0], Y_train.shape[0])\n",
    "    parameters = initialize_params(hyperparameters)\n",
    "    Z3 = forward_propagation(X, hyperparameters, parameters)\n",
    "    cost = compute_cost_with_regularization(Z3, Y,parameters,.1)\n",
    "    print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X_train, Y_train, minibatch_size, seed):\n",
    "    \"\"\"Tar in hela train och delar upp i x antal minibatch_size\n",
    "        med randomiserade kolumner (träningsexempel)\"\"\"\n",
    "    cuts = int(X_train.shape[1] / minibatch_size)\n",
    "    X_train, Y_train = shuffle(X_train, Y_train, seed)\n",
    "    minibatch_X = []\n",
    "    minibatch_Y = []\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0,cuts+1):\n",
    "        minibatches.append((X_train[:,i*minibatch_size:(i+1)*minibatch_size],Y_train[:,i*minibatch_size:(i+1)*minibatch_size]))\n",
    "\n",
    "    return minibatches\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.floor(np.random.rand(3,11)*10)\n",
    "minibatches = random_mini_batches(a,a,3,1)\n",
    "\n",
    "print(a)\n",
    "for (_,b) in minibatches:\n",
    "    pp.pprint(b)\n",
    "\n",
    "print(\"\\nbatch lengths: {0}\".format([b.shape[1] for (_,b) in minibatches]))\n",
    "print(sum([b.shape[1] for (_,b) in minibatches]))\n",
    "assert sum([b.shape[1] for (_,b) in minibatches]) == a.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell\n",
    "Här börjar vi om fast använder ovan funktioner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsondata = file_to_json(\"kabbel_new.txt\")\n",
    "ospeech, classes, person = kabbel_to_vec(jsondata)\n",
    "\n",
    "lem_dict = lemmatize(ospeech)\n",
    "speech = replace_words(ospeech, lem_dict)\n",
    "X_orig,vocabulary_x,iwx = samples_to_train(speech)\n",
    "Y_orig,vocabulary_y,iwy = samples_to_train(classes)\n",
    "\n",
    "#REMOVE STOP WORDS\n",
    "remove = 29\n",
    "print(\"{0} most used words:\".format(remove))\n",
    "del_idx = get_stop_words(X_orig,iwx,remove)\n",
    "X_orig, iwx_red = remove_features(X_orig,del_idx,iwx,remove)\n",
    "\n",
    "print(X_orig)\n",
    "row_sums = np.sum(X_orig,axis=1)\n",
    "print(iwx_red)\n",
    "#NORMALIZE X // not used\n",
    "#X_orig = X_orig / row_sums\n",
    "#print(X_orig)\n",
    "\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = split_train_dev_test(X_orig,Y_orig,ratio=.80)\n",
    "#print_data_info(speech,classes,test_index=0)\n",
    "\n",
    "print(vocabulary_y)\n",
    "print(\"train:{0} dev:{1} test:{2}\".format(X_train.shape,X_dev.shape,X_test.shape))\n",
    "\n",
    "#train:(18422, 19437) dev:(18422, 2430) test:(18422, 2430) (innan lemmatisering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#epoch = antal iterationer över samma samples fast med olika rand. dist. (som montecarlo eller nått)\n",
    "\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.1,\n",
    "          num_epochs = 3000, minibatch_size = 512, print_cost = True, lambd = 0.9):\n",
    "\n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results  \n",
    "    seed = 1                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "\n",
    "    \n",
    "    #skapa tomma placeholders för vår data\n",
    "    X, Y = create_placeholders(n_x,n_y)\n",
    "    CLR_learning_rate = tf.Variable(learning_rate,tf.float32)\n",
    "    \n",
    "    #skapa hyperparameter om lager och units samt aktiveringsfunktioner per lager ()\n",
    "    hyperparameters = init_hyperparams(layer_dims=[X_train.shape[0],100,20,5,4,Y_train.shape[0]])\n",
    "    #skapa W b i varje lager\n",
    "    parameters = initialize_params(hyperparameters)\n",
    "\n",
    "    #initiera beräkningsgrafen för forward/backward prop\n",
    "    Z3 = forward_propagation(X,hyperparameters,parameters)\n",
    "\n",
    "    #initiera beräkningsgraf för kostnaden \n",
    "    cost = compute_cost_with_regularization(Z3, Y, parameters, lambd) #compute_cost(Z3,Y)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    #nu med learning rate decay\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam').minimize(cost)\n",
    "   \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    epoch_print = 100\n",
    "    epoch_record = 20\n",
    "    e_count = 0\n",
    "    tot_e_count = num_epochs\n",
    "    tr_acc =[]\n",
    "    t_acc=[]\n",
    "     lr_cycl = [0.00001,.0001, 0.001,.01]\n",
    "    GoHomeYouAreDrunk = False\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        model_start_time = time.time()\n",
    "        last_time = model_start_time\n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            try:\n",
    "                epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "                num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "                seed +=1\n",
    "                minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "                cycl = len(minibatches)\n",
    "                minlr = 0.001\n",
    "                mbi = 0\n",
    "                for minibatch in minibatches:\n",
    "\n",
    "                        mbi +=1\n",
    "                        #lr = 0.001+(mbi*(maxlr-0.001)/cycl)\n",
    "                        #lr = 0.001+mbi*0.001\n",
    "                        # Select a minibatch\n",
    "                        (minibatch_X, minibatch_Y) = minibatch\n",
    "                        _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, \n",
    "                                                                                    Y: minibatch_Y,\n",
    "                                                                                    CLR_learning_rate: lr_cycl[mbi %3]})\n",
    "\n",
    "\n",
    "\n",
    "                        epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "                    \n",
    "\n",
    "                # Print the cost every epoch\n",
    "                if print_cost == True and (epoch % epoch_print == 0 and epoch > 0) or epoch == 10:\n",
    "\n",
    "                    t_now = time.time()\n",
    "                    e_count = epoch-e_count\n",
    "                    e_time = (t_now-last_time)/e_count\n",
    "\n",
    "                    est_end = (num_epochs - epoch)*e_time + t_now\n",
    "                    st = datetime.datetime.fromtimestamp(est_end).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    if epoch == 10:\n",
    "                        print(\"{0}\".format(datetime.timedelta(seconds=(est_end-t_now))))\n",
    "                    print(\"Cost after epoch {0}: {1} - epoch time {2} - est end: {3}\".format(epoch, epoch_cost,e_time,st))\n",
    "                    last_time = t_now\n",
    "                    t_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Z3), tf.argmax(Y)), \"float\"))\n",
    "\n",
    "                    tr_acc.append(t_accuracy.eval({X: X_train, Y: Y_train}))\n",
    "                    t_acc.append(t_accuracy.eval({X: X_test, Y: Y_test}))\n",
    "\n",
    "                    plt.plot(np.squeeze(tr_acc))\n",
    "                    plt.plot(np.squeeze(t_acc))\n",
    "                    plt.show()\n",
    "\n",
    "                elif print_cost == True and (epoch % epoch_record == 0):    \n",
    "                    t_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Z3), tf.argmax(Y)), \"float\"))\n",
    "                    tr_acc.append(t_accuracy.eval({X: X_train, Y: Y_train}))\n",
    "                    t_acc.append(t_accuracy.eval({X: X_test, Y: Y_test}))\n",
    "\n",
    "\n",
    "                if print_cost == True and epoch % 10 == 0:\n",
    "                    costs.append(epoch_cost)\n",
    "                    if costs[0] < epoch_cost:\n",
    "                        print(\"SOMETHING IS WRONG - cost increase?!\")\n",
    "                        break\n",
    "\n",
    "                if(GoHomeYouAreDrunk == True):\n",
    "                    tot_e_count = epoch\n",
    "                    print(\"break at epoch: \" + str(epoch))\n",
    "                    break\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"keyboard interrupt!!\")\n",
    "                GoHomeYouAreDrunk = True\n",
    "                break\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "        \n",
    "        tot_time = time.time() - model_start_time\n",
    "        print(\"time/epoch: \", tot_time/tot_e_count)\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        print(hyperparameters)\n",
    "        print(\"--- %s seconds ---\" % (tot_time))\n",
    "         # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "        return parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_shape_info(X_train, Y_train)\n",
    "parameters = model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001, num_epochs = 1000,lambd = 0.00008)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters have been trained!\n",
    "Train Accuracy: 0.989626\n",
    "Test Accuracy: 0.532374\n",
    "{'layer_dims': [3414, 10, 8, 5, 3, 5]}\n",
    "--- 76.7045350074768 seconds --- learning_rate = 0.001, num_epochs = 1000,lambd = 0.009 10,8,5,3,Y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def ran_vec(length,n_min,n_max,log=False):\n",
    "    vec = []\n",
    "    random.seed(2)\n",
    "    random.randint(1,1031)*10\n",
    "    np.random.randint(1, 13, size=length)\n",
    "    \n",
    "    for i in range(length):\n",
    "        vec[np.rand()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrate = []\n",
    "for l in range(2,6):\n",
    "    layer_dims = np.random.randint(1, 13, size=l)\n",
    "    learning_rate = np.random.exponential(1)\n",
    "    lrate.append(learning_rate)\n",
    "    print(learning_rate)\n",
    "    \n",
    "#learning_rate .1 .01 .001 (10)\n",
    "#layers 2-6 (4)\n",
    "#units 2, 3, 4, 5 (4)\n",
    "#lambd .9\n",
    "\n",
    "plt.plot(lrate)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.exponential(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Train Accuracy: 0.587367\n",
    "Test Accuracy: 0.282962\n",
    "{'layer_dims': [16506, 10, 5, 12]}\n",
    "\n",
    "number of training examples: 13283\n",
    "X_train shape: (16506, 13283)\n",
    "Y_train shape: (12, 13283)\n",
    "0:23:02.283483\n",
    "Cost after epoch 1: 2.547187099456787 - epoch time 6.946148157119751 - est end: 2017-09-21 17:02:05\n",
    "Cost after epoch 100: 0.399053031206131 - epoch time 3.540127908340608 - est end: 2017-09-21 16:50:47\n",
    "Parameters have been trained!\n",
    "Train Accuracy: 0.959723\n",
    "Test Accuracy: 0.268513\n",
    "{'layer_dims': [16506, 10, 5, 12]}\n",
    "\n",
    "Cost after epoch 0: 1.944496\n",
    "Cost after epoch 100: 0.072316\n",
    "Cost after epoch 200: 0.060759\n",
    "Parameters have been trained!\n",
    "Train Accuracy: 0.992783\n",
    "Test Accuracy: 0.503597\n",
    "{'layer_dims': [16506, 25, 12, 12]}\n",
    "--- 43.270642042160034 seconds ---\n",
    "\n",
    "number of training examples: 2217\n",
    "X_train shape: (5165, 2217)\n",
    "Y_train shape: (5, 2217)\n",
    "4\n",
    "ff1 W1xAi[0] + b1\n",
    "ff2 W2xAi[1] + b2\n",
    "ff3 W3xAi[2] + b3\n",
    "ff4 W4xAi[3] + b4\n",
    "Parameters have been trained!\n",
    "Train Accuracy: 0.983762\n",
    "Test Accuracy: 0.528777\n",
    "--- 143.9702799320221 seconds ---\n",
    "\n",
    "\n",
    "number of training examples: 2217\n",
    "X_train shape: (5165, 2217)\n",
    "Y_train shape: (5, 2217)\n",
    "4\n",
    "ff1 W1xAi[0] + b1\n",
    "ff2 W2xAi[1] + b2\n",
    "ff3 W3xAi[2] + b3\n",
    "ff4 W4xAi[3] + b4\n",
    "Cost after epoch 0: 2.557675\n",
    "Cost after epoch 20: 2.010333\n",
    "Cost after epoch 40: 1.836701\n",
    "Cost after epoch 60: 1.696013\n",
    "Cost after epoch 80: 1.532787\n",
    "\n",
    "Parameters have been trained!\n",
    "Train Accuracy: 0.820478\n",
    "Test Accuracy: 0.57554\n",
    "--- 14.365275144577026 seconds ---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test av modellen på ny data\n",
    "en predict-funktion som kör modellens forward-pass och använder softmax för klassificering till någon av de giltiga Y-värdena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(speech, parameters, vocabulary):\n",
    "        sample_to_vec(speech, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X_samp):\n",
    "    init = tf.global_variables_initializer()\n",
    "    X = tf.placeholder(shape=[X_train.shape[0],None],dtype=tf.float32,name=\"X\")\n",
    "    pred = forward_propagation(X,hyperparameters,parameters)\n",
    "    classify = tf.nn.softmax(tf.transpose(pred))\n",
    "    session = tf.Session()\n",
    "    session.run(init)\n",
    "    b = session.run(classify, feed_dict={X: X_samp})\n",
    "    return(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#XS_train, XS_dev, XS_test, YS_train, YS_dev, YS_test = split_train_dev_test(speech,classes,ratio=.82)\n",
    "sample_i = 1222\n",
    "b = predict(X_test[:,sample_i])\n",
    "c = np.zeros(b.shape[1]).astype(int)\n",
    "\n",
    "c[np.argmax(b)] = 1\n",
    "print(speech[sample_i])\n",
    "print(iwy[np.argmax(b)])\n",
    "print(classes[(sample_i-3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def samples_to(speech,classes,idx):\n",
    "    xvocabulary, iwx = samples_to_vocab(speech)\n",
    "    yvocabulary, iwy = samples_to_vocab(classes)\n",
    "    \n",
    "    _, x = sample_to_vec(speech[idx], xvocabulary)\n",
    "    _, y = sample_to_vec(classes[idx], yvocabulary)\n",
    "    return speech[idx],classes[idx],x.T,y, iwx,iwy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_vec(sentence,vocabular):\n",
    "    sample_words = np.array(sentence.split(\" \"))\n",
    "    vec = np.array([vocabular.get(x) for x in sample_words if vocabular.get(x) is not None])\n",
    "    one_hot = np.zeros([len(vec), len(vocabular)])\n",
    "    one_hot[np.arange(len(vec)),vec] = 1\n",
    "    one_hot = np.sum(one_hot,axis=0)\n",
    "    return one_hot, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_predict(sentence,full_vocabulary,classY):\n",
    "    str_s = sentence.lower()\n",
    "    jvoc = np.asarray([k for k,v in full_vocabulary.items()])\n",
    "    print(jvoc.shape)\n",
    "    jvoc, iwx_red = remove_features(jvoc,del_idx,iwx,remove)\n",
    "\n",
    "    jvoc = dict(zip(jvoc,range(len(jvoc))))\n",
    "    \n",
    "    xr,_ =sentence_to_vec(str_s,jvoc)\n",
    "    xr = xr.reshape(xr.shape[0],1)\n",
    "    pred = predict(xr)\n",
    "    c = np.zeros(pred.shape[1]).astype(int)\n",
    "    c[np.argmax(pred)] = 1\n",
    "    \n",
    "    print(\"{0}\\n{1} ---> {2}\\n\\n\\n{3}\\n\".format(str_s,x,pred,classY[np.argmax(pred)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X_orig.shape)\n",
    "str_s,str_c,x,y,iwx,iwy =    samples_to(speech,classes,1)\n",
    "sample_i = 2622\n",
    "\n",
    "jvoc = np.asarray([k for k,v in vocabulary_x.items()])\n",
    "print(jvoc.shape)\n",
    "jvoc, iwx_red = remove_features(jvoc,del_idx,iwx,remove)\n",
    "\n",
    "jvoc = dict(zip(jvoc,range(len(jvoc))))\n",
    "\n",
    "test_predict(\"solidaritet\",vocabulary_x,iwy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SD (?)\n",
    "Ok. inte klart ännu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
